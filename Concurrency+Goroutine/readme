Parallelism is not concurrency.
Parallelism is about running multiple tasks at the same time. Though you get better speed but you also need to provide that many resources to run tasks in parallel. A CPU core runs only one task at a time and with parallelism you will need n cores to run n tasks

Concurrency comes into play when you have to maintain an order of tasks. So as soons as Task1 gets completed on Core1, Task2 is picked by the idle Core2. Meanwhile Core1 is free to pickup Task3. Things go hand in hand one after the other in a concurrent fashion!

Now once utilization of CPU cores is optimized, next deadlock is memory. Caching helps to improve memory performance. Rather than depending on Main Memory, responses are served from Cache Memory.

two tasks are concurrent if the start time and the end time of the tasks overlap.

An OS performs processes concurrently. The switch between processes happens every 20ms in general. It happens so fast that we feel the processes are running in parallel even when they're not!
Deciding which process runs at which time, that's called the scheduling task and operating systems do that.

First-Come, First-Served (FCFS) Scheduling
Shortest-Job-Next (SJN) Scheduling
Priority Scheduling
Shortest Remaining Time
Round Robin(RR) Scheduling
Multiple-Level Queues Scheduling

Now, when the operating system moves from one process to another, when it starts a process running and then it starts another process running, that act of switching is called a context switch.

So, when you do a context switch between two processes, that might take you a long time because there's a lot of context, this unique. But with two threads, when you go from one thread to the next in the same process, it's much faster, because there's less contexts, less data that you have to write to memory and read back from memory when you do a context switch. So, and now what happens in operating systems is that they instead of scheduling processes, they scheduled thread by thread.





######################### GOROUTINES #########################


Goroutines are basically a thread but in Go. Many Goroutines execute within a single operating system thread. 

within Go, you can have multiple Goroutines that are executing within a main thread all alternating within that main thread.

Go can start switching basically these Goroutines which are like threads inside the main thread. So, that process of doing the switching determining which Goroutine executing and what time, that scheduling process, that's done by the Go Runtime Scheduler.

go func() // this is how a goroutine func is created by using the keyword go. The main() function is also a builtin goroutine. Important thing to note is if the main() goroutine completes it's execution then the other goroutines running within it will be forced to exit no matter if the child goroutines have finished their execution or not.



######################### INTERLEAVING #########################
it's the order of tasks performed. It keeps varying in parallel programming and is hard to track, i.e. which task ran before which task.
the interleaving, remember, is non-deterministic, it's determined by the operating system and the Go-runtime.


######################### RACE CONDITIONS ######################### 
So, a race condition is a problem that can happen as a result of all these possible interleaving that you have to consider.
Whenever two concurrent tasks use a shared resource, there are chances of race condition and we must avoid it. It can be checked like so : go run -race main.go
This will show if there is a Data race.




######################### SYCHRONIZATION ######################### 
This is needed so we can order the interleavings based on some global event so that we can have DETERMINISTIC output.
Example a task triggers a global event after it has done it's processing, and another task can run only once the global event has triggered. This way we can have some control over these tasks. But this is defeating concurrency, since the other task can't run until the global event is triggered. Like with concurrency, if a thread is ever waiting blocked waiting, then since the order doesn't matter, the scheduler can just move in another thread in the meantime while the first one's waiting. 

So understand that every time we use synchronization, we are reducing the efficiency, we're reducing the amount of possible concurrency. So we're reducing the options for the scheduler so the scheduler won't be able to use the hardware as well as effectively. So there might be times when nothing can execute because we're waiting on the synchronization events. So in general, synchronization is bad because it reduces your performance and your efficiency in general but it is necessary for cases like this where you have to restrict the ordering.





######################### WAIT GROUPS ######################### 
This module is a part of sync package. (import sync)
WaitGroups are useful for synchronization.
sync.WaitGroup is a group of goroutines that your goroutine is waiting on. For your goroutine to run, all the goroutines in the WaitGroup should have finished execution.
you can have one goroutine waiting on any no. of different goroutines and wait till they all complete.
sync.WaitGroup uses a counter to do the waiting. If let's say you have to wait for 5 goroutines to finish, the sync.WaitGroup counter will be increased from 0 to 5 and then as each of these goroutines finish their execution the counter is decreased by 1. Once the counter is back to 0 value, the waiting is complete.
There are methods like Add(), Done() and Wait() that help us achieve this waiting mechanism.
Add() -> This increments the counter value by one. If the counter becomes zero, all goroutines blocked on Wait are released. If the counter goes negative, Add panics.
Done() -> Done decrements the WaitGroup counter by one. The blocking goroutines call this method at the end of their code.
Wait() -> Wait blocks until the WaitGroup counter is zero.


It's up to the programmer to inject the Add, the Done in the Wait at the right places to make this whole mechanism work.





Example:

func foo(wg *sync.WaitGroup) {  // this is the goroutine I want to execute before main() goroutines finishes
	fmt.Printf("New routine")
	wg.Done()   // Done() signifies that foo() is done with it's execution
}

func main() {
	var wg sync.WaitGroup  // initializing the WaitGroup
	wg.Add(1)	// adding 1 to the counter since I want to wait for 1 goroutine i.e. foo()
	go foo(&wg)	// here I call the goroutine foo() and pass pointer to WaitGroup variable
	wg.Wait()	// here I Wait() until all goroutines are Done()
	fmt.Printf("Main routine") // lastly this gets printed
}





######################### COMMUNICATION B/W GOROUTINES ######################### 
Goroutines are generally layered out from a main program. The data between goroutines is often transferred and hence Go has Channels for this communication between goroutines.
Channels are used to transfer data between goroutines and channels are typed. When you create a channel, you create it with a certain type.

Example:
channel1 := make(chan int) // this channel is used to transfer int type of data

channel1 <- 3 // to send data to channel

x := <- channel1 // to receive data from channel


Example:
func multiply(v1 int, v2 int, c chan int) {  // this takes 2 integer vars and 1 int type channel
	c <- v1 * v2 // stores the result in channel
}

func main(){
	c := make(chan int) // declare the channel
	go multiply(1,2,c) // call the goroutine and pass the channel
	go multiply(3,4,c) // call the goroutine and pass the channel
	a := <- c // store the result from channel to a variable
	b := <- c // store the result from channel to a variable
	fmt.Println(a*b)
}




Now the above example of channel was when the data was not coming in transit. If the data is coming in transit to the channel, we need to buffer it.
So the implications are that, since we don't want to lose data, the transmission or sending instruction has to block until the data is received on the receiving end, and the receiving instruction has to block until the data is sent.

By default channels cannot hold data and there might be data loss in an unbuffered channel.







######################### BUFFERRED CHANNELS ######################### 
So by default, a channel is called Unbuffered. When you create a channel, it's unbuffered, and unbuffered channels cannot hold data in transit. 
To make bufferred channels you need to give them some capacity to hold data. By default the capacity is zero.

buff_chan := make(chan int,3) // this channel has a capacity of 3. So it will block the sender from the 4th send request since the buffer is full and at capacity.
Receiver gets blocked if the buffer is empty. So it will block the 4th receive since it has only 3 items.

Buffering is useful so that sender and receiver don't have to be in sync and the data can be stored in the buffer.





Iterating over a channel can be done with a range keyword on the for loop. But it becomes important now to close(c) otherwise the for loop will be infinite since it won't know if the data has stopped coming into the channel.

for i := range c {
	fmt.Println(i)
}

close(c)


######################### SELECT ######################### 
When you want to have the ability to choose from the multiple channels. You maybe want to go with FCFS for multiple channels.


Example you dont want to wait for c1 or c2 but want to continue on availability case:

select {
	case a = <- c1:
		fmt.Println(a)
	case b = <- c2:
		fmt.Println(b)
}

You can select on both sending and receiving the channels. Example:

select {
	case a = <- in_channel:
		fmt.Println("Received a")
	case out_channel <- b:
		fmt.Println("Sent b")
	case <- abort :    // you can stop executing once an abort signal is received on the "abort" channel
		return
	default:
		fmt.Println("you can also have a default if nothing matches")
}





######################### MUTEX ######################### 
 how we do sharing of data correctly between goroutines? Don't let two goroutines write to a shared variable at the same time, just as a rule of thumb. Don't let that happen because you can get these interleavings that are going to cause issues.
Then you have to restrict their interleavings in such a way that you make sure that they can't both write to it at the same time. So, access to the shared variables cannot be interleaved. Mutex is used for this purpose.

A Mutex ensure mutual exclusion via binary semaphores. lock() and unlock() are used.


Example:

var i int = 0
var mut sync.Mutex
func inc() {
	mut.Lock() // so until unlock() is called no other goroutine can work on the global variable i
	i += 1
	mut.Unlock()
}



If you are running multiple goroutines of the same function and want some piece of code to be run only once no matter how many times it's called by goroutines.. you can use sync.Once

Example:

var wg sync.WaitGroup

func main() {
	wg.Add(2) // 2 goroutines
	go dostuff() // called func
	go dostuff() // called the same func again
	wg.Wait() // wait for the goroutines to finish
}

var on sync.Once

func setup() {
	fmt.Println("Should be called only once")
}

func dostuff() {
	on.Do(setup)  // setup() will be called but only once
	fmt.Println("This will be called as many times the goroutine is called")
	wg.Done() // finish the goroutine
}
